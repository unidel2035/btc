"""
Sentiment Analysis –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –Ω–∞ FastAPI
"""
import time
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from sentiment_analyzer import SentimentAnalyzer

# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI
app = FastAPI(
    title="Sentiment Analysis API",
    description="NLP –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
analyzer = SentimentAnalyzer()


class AnalyzeRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑"""
    text: str = Field(..., description="–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    type: Optional[str] = Field("news", description="–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: news, social, other")


class EntityInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—É—â–Ω–æ—Å—Ç–∏"""
    text: str
    type: str
    start: int
    end: int


class AnalyzeResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    sentiment: float = Field(..., description="–û—Ü–µ–Ω–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç -1 (negative) –¥–æ 1 (positive)")
    confidence: float = Field(..., description="–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0-1)")
    label: str = Field(..., description="–ú–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞: positive, negative, neutral")
    entities: List[EntityInfo] = Field(default_factory=list, description="–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏")
    impact: str = Field(..., description="–£—Ä–æ–≤–µ–Ω—å –≤–∞–∂–Ω–æ—Å—Ç–∏: high, medium, low")
    keywords: List[str] = Field(default_factory=list, description="–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
    processing_time: float = Field(..., description="–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –º—Å")


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    print("üöÄ Starting Sentiment Analysis API...")
    print("üìä Loading models...")
    # –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ SentimentAnalyzer
    print("‚úÖ Models loaded successfully")


@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "service": "Sentiment Analysis API",
        "version": "1.0.0",
        "status": "running"
    }


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "healthy",
        "model": analyzer.model_name,
        "spacy_model": analyzer.nlp.meta["name"]
    }


@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_text(request: AnalyzeRequest):
    """
    –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è

    Args:
        request: –ó–∞–ø—Ä–æ—Å —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        AnalyzeResponse: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    if not request.text or not request.text.strip():
        raise HTTPException(status_code=400, detail="Text cannot be empty")

    try:
        start_time = time.time()

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
        result = analyzer.analyze(request.text, content_type=request.type)

        processing_time = (time.time() - start_time) * 1000  # –í –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

        return AnalyzeResponse(
            sentiment=result["sentiment"],
            confidence=result["confidence"],
            label=result["label"],
            entities=[EntityInfo(**entity) for entity in result["entities"]],
            impact=result["impact"],
            keywords=result["keywords"],
            processing_time=processing_time
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


@app.post("/batch", response_model=List[AnalyzeResponse])
async def batch_analyze(requests: List[AnalyzeRequest]):
    """
    –ë–∞—Ç—á-–∞–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤

    Args:
        requests: –°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤

    Returns:
        List[AnalyzeResponse]: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    if not requests:
        raise HTTPException(status_code=400, detail="Request list cannot be empty")

    if len(requests) > 100:
        raise HTTPException(status_code=400, detail="Maximum 100 texts per batch")

    results = []
    for req in requests:
        try:
            start_time = time.time()
            result = analyzer.analyze(req.text, content_type=req.type or "news")
            processing_time = (time.time() - start_time) * 1000

            results.append(AnalyzeResponse(
                sentiment=result["sentiment"],
                confidence=result["confidence"],
                label=result["label"],
                entities=[EntityInfo(**entity) for entity in result["entities"]],
                impact=result["impact"],
                keywords=result["keywords"],
                processing_time=processing_time
            ))
        except Exception as e:
            # –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            results.append(AnalyzeResponse(
                sentiment=0.0,
                confidence=0.0,
                label="neutral",
                entities=[],
                impact="low",
                keywords=[],
                processing_time=0.0
            ))

    return results


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
